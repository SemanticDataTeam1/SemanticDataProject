{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5a07b0-b3ba-4a4b-b094-c7a268ec924e",
   "metadata": {},
   "source": [
    "candidates_dict_algo = {\"svm\" : \"SVM_Support_Vector_Machine\",\n",
    "                       \"support vector machine\" : \"SVM_Support_Vector_Machine\",\n",
    "                       \"rnn\" : \"RNN_Recurrent_Neural_Network\",\n",
    "                       \"recurrent neural network\" : \"RNN_Recurrent_Neural_Network\",\n",
    "                       \"fnn\" : \"FNN_FeedForward_Neural_Network\",\n",
    "                       \"feedforward neural network\" : \"FNN_FeedForward_Neural_Network\",\n",
    "                       \"logistic regression\" : \"Logistic_Regression\",\n",
    "                       \"agglomerative clustering\" : \"Agglomerative_Clustering\",\n",
    "                       \"cox\" : \"Cox_Proportional_Hazards_Algorithm\",\n",
    "                       \"proportional hazards algorithm\" : \"Cox_Proportional_Hazards_Algorithm\",\n",
    "                       \"dbscan\" : \"DBSCAN\",\n",
    "                       \"decision tree\" : \"Decision Tree\",\n",
    "                       \"hdbscan\" : \"HDBSCAN\",\n",
    "                       \"isolation forest\" : \"Isolation_Forest\",\n",
    "                       \"k-means\" : \"K-Means\",\n",
    "                       \"k-nearest neighbor\" : \"KNN\",\n",
    "                        \"knn\" : \"KNN\",\n",
    "                       \"kaplan-meier\" : \"Kaplan-Meier\",\n",
    "                       \"lightgbm\" : \"LightGBM\",\n",
    "                       \"lda\" : \"LDA_Linear_Discriminant_Analysis\",\n",
    "                       \"linear discriminant analysis\" : \"LDA_Linear_Discriminant_Analysis\",\n",
    "                       \"linear regression\" : \"Linear_Regression\",\n",
    "                       \"lle\" : \"LLE_Locally_Linear_Embedding\",\n",
    "                       \"locally linear embedding\" : \"LLE_Locally_Linear_Embedding\",\n",
    "                        \"mean shift\" : \"Mean_Shift\",\n",
    "                        \"naive bayes\" : \"Naive_Bayes\",\n",
    "                        \"nelson-aalen\" : \"Nelson-Aalen\",\n",
    "                        \"one-class svm\" : \"One-Class_SVM\",\n",
    "                        \"one-class svm with sgd\" : \"One-Class_SVM_With_SGD\",\n",
    "                        \"random forest\" : \"Random_Forest\",\n",
    "                        \"robust covariance\" : \"Robust_Covariance\"\n",
    "                       }\n",
    "\n",
    "candidates_dict_optim_algo = {\n",
    "    \"backpropagation\" : \"Backpropagation\",\n",
    "    \"catboost\" : \"CatBoost\",\n",
    "    \"ensemble optmization algorithm\" : \"Ensemble_Optimization_Algorithm\",\n",
    "    \"adaboost\" : \"AdaBoost\",\n",
    "    \"expectation maximization\" : \"EM_Expectation-Maximization\",\n",
    "    \"gradient descent\" : \"Gradient_Descent\",\n",
    "    \"isolation tree\" : \"Isolation_Tree\",\n",
    "    \"isomap\" : \"Isomap\",\n",
    "    \"pca\" : \"PCA\",\n",
    "    \"sgd\" : \"SGD_Stochastic_Gradient_Descent\",\n",
    "    \"stochastic gradient descent\" : \"SGD_Stochastic_Gradient_Descent\",\n",
    "    \"t-sne\" : \"t-SNE\",\n",
    "    \"umap\" : \"UMAP\",\n",
    "    \"xgboost\" : \"XGBoost\"\n",
    "}\n",
    "\n",
    "candidates_dict_supervision = {\n",
    "    'self-supervised learning': 'Self_Supervised_Learning',\n",
    "    'semi-supervised learning': 'Semi-Supervised_Learning',\n",
    "    'supervised learning': 'Supervised_Learning',\n",
    "    'unsupervised learning': 'Unsupervised_Learning'\n",
    "}\n",
    "\n",
    "candidates_dict_core_ml_task = {\n",
    "    'anomaly detection task': 'Anomaly_Detection_Task',\n",
    "    'anomaly detection' : 'Anomaly_Detection_Task',\n",
    "    'classification task': 'Classification_Task',\n",
    "    'classification' : 'Classification_Task',\n",
    "    'clustering task': 'Clustering_Task',\n",
    "    'clustering' : 'Clustering_Task',\n",
    "    'regression task': 'Regression_Task',\n",
    "    'regression' : 'Regression_Task'\n",
    "}\n",
    "\n",
    "candidates_dict_complex_task = {\n",
    "    'computer vision task': 'ComputerVision_Task',\n",
    "    'computer vision' : 'ComputerVision_Task',\n",
    "    'decision making task': 'Decision_Making_Task',\n",
    "    'decision making' : 'Decision_Making_Task',\n",
    "    'deep learning task': 'Deep_Learning_Task',\n",
    "    'deep learning' : 'Deep_Learning_Task',\n",
    "    'evolutionay escape mutations': 'Evolutionay_Escape_Mutations',\n",
    "    'media generation task': 'Media_Generation_Task',\n",
    "    'media generation' : 'Media_Generation_Task',\n",
    "    'nlp task': 'NLP_Task',\n",
    "    'nlp' : 'NLP_Task',\n",
    "    'sequence evolution escape task': 'Sequence_Evolution_Escape_Task',\n",
    "    'ml' : 'Machine_Learning_Task',\n",
    "    'machine learning task' : 'Machine_Learning_Task',\n",
    "    'machine learning' : 'Machine_Learning_Task'\n",
    "}\n",
    "\n",
    "candidates_dict_application_domain = {\n",
    "    'agriculture' : 'Agriculture',\n",
    "    'automotive' : 'Automotive',\n",
    "    'cybersecurity' : 'Cybersecurity',\n",
    "    'education' : 'Education',\n",
    "    'energy and utilities' : 'Energy_And_Utilities',\n",
    "    'energy' : 'Energy_And_Utilities',\n",
    "    'utilities' : 'Energy_And_Utilities',\n",
    "    'finance' : 'Finance',\n",
    "    'healthcare' : 'Healthcare',\n",
    "    'antibiotic research' : 'Antibiotic_Reasearch',\n",
    "    'antibiotic' : 'Antibiotic_Reasearch',\n",
    "    'antibiotics' : 'Antibiotic_Reasearch',\n",
    "    'cancer detection' : 'Cancer_Detection',\n",
    "    'pathogenic evolution' : 'Pathogenic_Evolution',\n",
    "    'manufacturing' : 'Manufacturing',\n",
    "    'media' : 'Media_and_Entertainment',\n",
    "    'entertainment' : 'Media_and_Entertainment',\n",
    "    'retail' : 'Retail_and_E-commerce',\n",
    "    'e-commerce' : 'Retail_and_E-commerce',\n",
    "    'robotics' : 'Robotics',\n",
    "    'smart cities' : 'Smart_Cities_and_IoT',\n",
    "    'iot' : 'Smart_Cities_and_IoT',\n",
    "    'internet of things' : 'Smart_Cities_and_IoT',\n",
    "    'telecoms' : 'Telecommunications',\n",
    "    'telecommunication' : 'Telecommunications',\n",
    "    'telecommunications' : 'Telecommunications',\n",
    "}\n",
    "\n",
    "candidates_dict_preprocessing = {\n",
    "    'data augmentation': 'Data_Augmentation',\n",
    "    'dimensionality reduction': 'Dimensionality_Reduction',\n",
    "    'encoding': 'Encoding',\n",
    "    'feature engineering': 'Feature_Engineering',\n",
    "    'feature extraction': 'Feature_Extraction',\n",
    "    'feature selection': 'Feature_Selection',\n",
    "    'imputation': 'Imputation',\n",
    "    'normalization': 'Normalization',\n",
    "    'outlier detection and removal': 'Outlier_Detection_And_Removal',\n",
    "    'outlier detection' : 'Outlier_Detection_And_Removal',\n",
    "    'standardization': 'Standardization'\n",
    "}\n",
    "\n",
    "candidates_dict_learningtype = {\n",
    "    \"ensemble learning\" : \"Ensemble_Learning\",\n",
    "    \"federated learning\" : \"Federated_Learning\",\n",
    "    \"few shot learning\" : \"Few_Shot_Learning\",\n",
    "    \"meta learning\" : \"Meta_Learning\",\n",
    "    \"reinforcement learning\" : \"Reinforcement_Learning\",\n",
    "    \"zero shot learning\" : \"Zero_Shot_Learning\"\n",
    "}\n",
    "\n",
    "candidates = list(candidates_dict_algo.keys()) + list(candidates_dict_optim_algo.keys()) + list(candidates_dict_supervision.keys()) + list(candidates_dict_core_ml_task.keys()) + list(candidates_dict_complex_task.keys()) + list(candidates_dict_application_domain.keys()) + list(candidates_dict_preprocessing.keys()) + list(candidates_dict_learningtype.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd6fa17-caf2-4912-a5b2-68eebfed45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yield successive n-sized\n",
    "# chunks from l.\n",
    "def divide_chunks(l, n):\n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    "\n",
    "#title of articles we want to scrape\n",
    "list_of_authors = [\"Alan Turing\", \"Geoffrey Hinton\", \"Yann LeCun\", \"Yoshua Bengio\", \"Andrew Ng\", \"Fei-Fei Li\", \"Ian Goodfellow\", \"Judea Pearl\", \"Michael I. Jordan\", \"Rajeev Rastogi\", \"Daphne Koller\", \"Christopher Bishop\", \"Sergey Levine\", \"Zoubin Ghahramani\", \"Richard S. Sutton\", \"Lex Fridman\", \"Demis Hassabis\", \"Ruslan Salakhutdinov\", \"Ali Farhadi\", \"David Silver\", \"Samy Bengio\", \"Shakir Mohamed\", \"Andrew Gelman\", \"Padhraic Smyth\", \"Tom Mitchell\", \"Silvio Savarese\", \"Leslie Kaelbling\"]\n",
    "list_of_list_of_authors = list(divide_chunks(list_of_authors, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38756541-6472-4c0f-9073-fbb2145a980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shroudedsublet/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-20 17:25:30.939852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732123530.980489    1664 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732123530.991110    1664 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-20 17:25:31.025270: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 μs, sys: 0 ns, total: 11 μs\n",
      "Wall time: 21.5 μs\n",
      "Num of batch:  0\n",
      "Num of batch:  1\n",
      "Num of batch:  2\n",
      "Num of batch:  3\n",
      "Num of batch:  4\n",
      "Num of batch:  5\n"
     ]
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from scholarly import ProxyGenerator\n",
    "\n",
    "%time\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# number of papers per author\n",
    "num_paper_per_author = 25\n",
    "\n",
    "#columns for csv file\n",
    "cols = [\"url\", \"title\", \"abstract\", \"published_date\", \"authors\", \"num_cites\", \n",
    "        \"prediction_algo\", \"optim_algo\", \"supervision\", \"core_task\", \"complex_task\", \n",
    "        \"application_domain\", \"preprocessing\", \"learningtype\"]\n",
    "\n",
    "rows = []\n",
    "keywords = []\n",
    "df = []\n",
    "\n",
    "list_of_pub = []\n",
    "\n",
    "for j in range(0,len(list_of_list_of_authors)):\n",
    "    print(\"Num of batch: \", j)\n",
    "    for i in list_of_list_of_authors[j]:\n",
    "\n",
    "        # Retrieve the author's data, fill-in, and print\n",
    "        # Get an iterator for the author results\n",
    "        search_query = scholarly.search_author(i)\n",
    "        # Retrieve the first result from the iterator\n",
    "        first_author_result = next(search_query)\n",
    "        # Retrieve all the details for the author\n",
    "        author = scholarly.fill(first_author_result)\n",
    "        \n",
    "        # Take a closer look at the publication\n",
    "        for k in range(0,num_paper_per_author):\n",
    "            publication = author['publications'][k]\n",
    "            scholarly.fill(publication)\n",
    "\n",
    "        # Print the titles of the author's publications\n",
    "        for index,pub in enumerate (author['publications']):\n",
    "            try:\n",
    "                title = pub['bib']['title']\n",
    "                author = pub['bib']['author'].split(\" and \")\n",
    "                published_year = pub['bib']['pub_year']\n",
    "                abstract = pub['bib']['abstract']\n",
    "                url = pub['pub_url']\n",
    "                num_cites = pub['num_citations']\n",
    "                if index == num_paper_per_author-1:\n",
    "                    break\n",
    "            except KeyError:\n",
    "                continue\n",
    "    \n",
    "            keyword = kw_model.extract_keywords(abstract.lower(), \n",
    "                                                candidates=candidates, \n",
    "                                                keyphrase_ngram_range=(1, 4), \n",
    "                                                top_n=10)\n",
    "\n",
    "            #separate authors\n",
    "            for x in author:\n",
    "                x = x.replace(\" \", \"_\")\n",
    "                row = [url, title, abstract, published_year, x, num_cites, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "                rows.append(row)\n",
    "    \n",
    "            #sort keywords\n",
    "            for x in range(1, len(keyword)):\n",
    "                if keyword[x][0] in candidates_dict_algo:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, candidates_dict_algo[keyword[x][0]], \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_optim_algo:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", candidates_dict_optim_algo[keyword[x][0]], \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_supervision:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", candidates_dict_supervision[keyword[x][0]], \"\", \"\", \"\", \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_core_ml_task:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", \"\", candidates_dict_core_ml_task[keyword[x][0]], \"\", \"\", \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_complex_task:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", \"\", \"\", candidates_dict_complex_task[keyword[x][0]], \"\", \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_application_domain:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", \"\", \"\", \"\", candidates_dict_application_domain[keyword[x][0]], \"\", \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_preprocessing:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", \"\", \"\", \"\", \"\", candidates_dict_preprocessing[keyword[x][0]], \"\"]\n",
    "                    rows.append(row)\n",
    "                elif keyword[x][0] in candidates_dict_learningtype:\n",
    "                    row = [url, title, abstract, published_year, \"\", num_cites, \"\", \"\", \"\", \"\", \"\", \"\", \"\", candidates_dict_learningtype[keyword[x][0]]]\n",
    "                    rows.append(row)\n",
    "\n",
    "    df.append(pd.DataFrame(rows, columns=cols))\n",
    "\n",
    "    csv_title = \"df\" + str(j) + \".csv\"\n",
    "    file_obj1 = open(csv_title, 'w')\n",
    "    df[j].to_csv(csv_title, encoding='utf-8', index=False)\n",
    "    file_obj1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68249a4-bdb1-4ca1-b94c-5dd92f222191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./df5.csv', './df4.csv', './df0.csv', './df1.csv', './df2.csv', './df3.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv, glob\n",
    "\n",
    "Dir = r\"./\"\n",
    "Avg_Dir = r\"./\"\n",
    "\n",
    "csv_file_list = glob.glob(os.path.join(Dir, '*.csv')) # returns the file list\n",
    "print (csv_file_list)\n",
    "\n",
    "with open(os.path.join(Avg_Dir, 'Output.csv'), 'w', newline='') as f:\n",
    "    wf = csv.writer(f, lineterminator='\\n')\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for files in csv_file_list:\n",
    "        with open(files, 'r') as r: \n",
    "            if i != 0:\n",
    "                next(r)                   # SKIP HEADERS\n",
    "                i = 1\n",
    "            rr = csv.reader(r)\n",
    "            for row in rr:\n",
    "                wf.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29faacba-f218-423a-9163-a0c1169ae324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipykernel_1664/4026981461.py:11: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  cleanString = re.sub('\\W+','_', string)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Output.csv\")\n",
    "\n",
    "import re\n",
    "\n",
    "def toStr(string):\n",
    "    return str(string)\n",
    "\n",
    "df['authors'] = df['authors'].apply(toStr)\n",
    "\n",
    "def clean(string):\n",
    "    cleanString = re.sub('\\W+','_', string)\n",
    "    return cleanString\n",
    "\n",
    "df['title']= df['title'].apply(clean)\n",
    "df['authors'] = df['authors'].apply(clean)\n",
    "\n",
    "df.to_csv(\"clean_output.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d77ce1-c292-4387-b805-25b83cb00e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30009 entries, 0 to 30008\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   url                 30009 non-null  object\n",
      " 1   title               30009 non-null  object\n",
      " 2   abstract            30009 non-null  object\n",
      " 3   published_date      30009 non-null  object\n",
      " 4   authors             30009 non-null  object\n",
      " 5   num_cites           30009 non-null  object\n",
      " 6   prediction_algo     60 non-null     object\n",
      " 7   optim_algo          100 non-null    object\n",
      " 8   supervision         58 non-null     object\n",
      " 9   core_task           238 non-null    object\n",
      " 10  complex_task        165 non-null    object\n",
      " 11  application_domain  84 non-null     object\n",
      " 12  preprocessing       41 non-null     object\n",
      " 13  learningtype        13 non-null     object\n",
      "dtypes: object(14)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd25521-a598-4811-8ab6-79c9eeb2eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
